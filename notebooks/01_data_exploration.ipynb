{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHANES 2017-2018 データ探索\n",
    "\n",
    "## 目的\n",
    "- NHANESデータの基本統計量を把握\n",
    "- 欠損値パターンを分析\n",
    "- 重要変数の分布を確認\n",
    "- 異常値の検出\n",
    "\n",
    "## データ概要\n",
    "- 参加者数: 9,254名\n",
    "- データソース: NHANES 2017-2018\n",
    "- 評価項目: 心血管、代謝、腎、肝、血液マーカー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# プロジェクトルートをパスに追加\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.data.loader import NHANESLoader\n",
    "from src.data.preprocessor import NHANESPreprocessor\n",
    "from src.data.validator import DataValidator\n",
    "\n",
    "# 日本語フォント設定\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 表示設定\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# 図のスタイル\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダーの初期化\n",
    "data_dir = project_root / 'data' / 'raw'\n",
    "loader = NHANESLoader(data_dir)\n",
    "\n",
    "# CSVファイルのリストを表示\n",
    "csv_files = list(data_dir.glob('*.csv'))\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for f in csv_files:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み（最初のCSVファイルを読み込む）\n",
    "if csv_files:\n",
    "    df_raw = loader.load_csv(csv_files[0].name)\n",
    "    print(f\"\\nLoaded data shape: {df_raw.shape}\")\n",
    "    print(f\"Columns: {df_raw.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"No CSV files found. Please place data files in data/raw/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基本統計量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ情報の取得\n",
    "if 'df_raw' in locals():\n",
    "    info = loader.get_data_info(df_raw)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA INFORMATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Number of rows: {info['n_rows']:,}\")\n",
    "    print(f\"Number of columns: {info['n_columns']}\")\n",
    "    print(f\"Memory usage: {info['memory_usage_mb']:.2f} MB\")\n",
    "    print(\"\\nData types:\")\n",
    "    print(pd.Series(info['dtypes']).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本統計量\n",
    "if 'df_raw' in locals():\n",
    "    df_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 欠損値分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値の割合\n",
    "if 'df_raw' in locals():\n",
    "    missing_data = pd.DataFrame({\n",
    "        'Missing_Count': df_raw.isnull().sum(),\n",
    "        'Missing_Percentage': (df_raw.isnull().sum() / len(df_raw) * 100).round(2)\n",
    "    })\n",
    "    missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MISSING DATA SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(missing_data)\n",
    "    \n",
    "    # 欠損値の可視化\n",
    "    if len(missing_data) > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        missing_data['Missing_Percentage'].plot(kind='barh', color='salmon')\n",
    "        plt.xlabel('Missing Percentage (%)')\n",
    "        plt.title('Missing Data by Variable')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missingnoによる欠損値パターンの可視化\n",
    "if 'df_raw' in locals():\n",
    "    msno.matrix(df_raw, figsize=(14, 8), fontsize=10)\n",
    "    plt.title('Missing Data Pattern')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. データ前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理の実行\n",
    "if 'df_raw' in locals():\n",
    "    preprocessor = NHANESPreprocessor()\n",
    "    df_processed = preprocessor.preprocess(df_raw)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PREPROCESSING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    summary = preprocessor.get_preprocessing_summary(df_raw, df_processed)\n",
    "    print(f\"Original shape: {summary['original_shape']}\")\n",
    "    print(f\"Processed shape: {summary['processed_shape']}\")\n",
    "    print(f\"\\nNew columns added: {len(summary['new_columns'])}\")\n",
    "    for col in summary['new_columns']:\n",
    "        print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 人口統計的特徴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 年齢分布\n",
    "if 'df_processed' in locals() and 'RIDAGEYR' in df_processed.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ヒストグラム\n",
    "    axes[0].hist(df_processed['RIDAGEYR'].dropna(), bins=50, color='steelblue', edgecolor='black')\n",
    "    axes[0].set_xlabel('Age (years)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Age Distribution')\n",
    "    axes[0].axvline(df_processed['RIDAGEYR'].median(), color='red', linestyle='--', label='Median')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # 年齢グループ\n",
    "    if 'age_group' in df_processed.columns:\n",
    "        age_group_counts = df_processed['age_group'].value_counts().sort_index()\n",
    "        axes[1].bar(range(len(age_group_counts)), age_group_counts.values, color='lightgreen', edgecolor='black')\n",
    "        axes[1].set_xticks(range(len(age_group_counts)))\n",
    "        axes[1].set_xticklabels(age_group_counts.index, rotation=45)\n",
    "        axes[1].set_ylabel('Count')\n",
    "        axes[1].set_title('Age Group Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nAge statistics:\")\n",
    "    print(df_processed['RIDAGEYR'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性別分布\n",
    "if 'df_processed' in locals() and 'gender_label' in df_processed.columns:\n",
    "    gender_counts = df_processed['gender_label'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pie(gender_counts.values, labels=gender_counts.index, autopct='%1.1f%%', \n",
    "            colors=['lightblue', 'lightpink'], startangle=90)\n",
    "    plt.title('Gender Distribution')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nGender counts:\")\n",
    "    print(gender_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 重要変数の分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 脂質プロファイル\n",
    "if 'df_processed' in locals():\n",
    "    lipid_vars = ['LBXTC', 'LBDHDD', 'LBDLDL', 'LBXTR']\n",
    "    lipid_labels = ['Total Cholesterol', 'HDL', 'LDL', 'Triglycerides']\n",
    "    \n",
    "    existing_lipid_vars = [v for v in lipid_vars if v in df_processed.columns]\n",
    "    \n",
    "    if existing_lipid_vars:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i, (var, label) in enumerate(zip(existing_lipid_vars, lipid_labels[:len(existing_lipid_vars)])):\n",
    "            data = df_processed[var].dropna()\n",
    "            axes[i].hist(data, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "            axes[i].axvline(data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {data.mean():.1f}')\n",
    "            axes[i].axvline(data.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {data.median():.1f}')\n",
    "            axes[i].set_xlabel(f'{label} (mg/dL)')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].set_title(f'{label} Distribution')\n",
    "            axes[i].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代謝マーカー\n",
    "if 'df_processed' in locals():\n",
    "    metabolic_vars = ['LBXGLU', 'LBXGH', 'LBXIN']\n",
    "    metabolic_labels = ['Glucose', 'HbA1c', 'Insulin']\n",
    "    \n",
    "    existing_metabolic_vars = [v for v in metabolic_vars if v in df_processed.columns]\n",
    "    \n",
    "    if existing_metabolic_vars:\n",
    "        n_vars = len(existing_metabolic_vars)\n",
    "        fig, axes = plt.subplots(1, n_vars, figsize=(6*n_vars, 5))\n",
    "        if n_vars == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, (var, label) in enumerate(zip(existing_metabolic_vars, metabolic_labels[:n_vars])):\n",
    "            data = df_processed[var].dropna()\n",
    "            axes[i].hist(data, bins=50, color='orange', alpha=0.7, edgecolor='black')\n",
    "            axes[i].axvline(data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {data.mean():.1f}')\n",
    "            axes[i].set_xlabel(label)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].set_title(f'{label} Distribution')\n",
    "            axes[i].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 変数間相関分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 脂質マーカー間の相関\n",
    "if 'df_processed' in locals():\n",
    "    lipid_vars = ['LBXTC', 'LBDHDD', 'LBDLDL', 'LBXTR']\n",
    "    existing_lipid_vars = [v for v in lipid_vars if v in df_processed.columns]\n",
    "    \n",
    "    if len(existing_lipid_vars) > 1:\n",
    "        correlation_matrix = df_processed[existing_lipid_vars].corr()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                    square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "        plt.title('Lipid Profile Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. データ検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ品質検証\n",
    "if 'df_processed' in locals():\n",
    "    validator = DataValidator()\n",
    "    validation_results = validator.validate(df_processed)\n",
    "    \n",
    "    print(validator.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. データ保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理済みデータの保存\n",
    "if 'df_processed' in locals():\n",
    "    processed_dir = project_root / 'data' / 'processed'\n",
    "    processed_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    output_path = processed_dir / 'nhanes_processed.csv'\n",
    "    df_processed.to_csv(output_path, index=False)\n",
    "    print(f\"\\nProcessed data saved to: {output_path}\")\n",
    "    print(f\"Shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは:\n",
    "1. NHANESデータの読み込みと基本統計量の確認\n",
    "2. 欠損値パターンの分析\n",
    "3. データの前処理（欠損値処理、外れ値処理、派生変数生成）\n",
    "4. 人口統計的特徴の可視化\n",
    "5. 重要変数の分布確認\n",
    "6. 変数間相関の分析\n",
    "7. データ品質の検証\n",
    "8. 前処理済みデータの保存\n",
    "\n",
    "を実行しました。\n",
    "\n",
    "次のステップ:\n",
    "- 02_feature_engineering.ipynb: 追加の特徴量エンジニアリング\n",
    "- 03_risk_model_development.ipynb: リスクスコアモデルの開発とテスト"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
